---
title: "bibsearch"
format: html
editor: visual
---

```{r}
#| label: setup
library(tidyverse)
library(magrittr, include.only = "%<>%")
library(conflicted)

# maintain consistency -- within limits of implementation -- across databases

query.ls <- list(
           OR_terms = 
             c("impact","attitude","incentive","cost","benefit","risk","reward"),
           AND_terms = c("open science"),
           NOT_terms = c(),
           FROM_date = as.Date("2019-01-1"),
           UNTIL_date = as.Date("2024-12-30"),
           MAX_results = 3000,
           EMAIL_str = "foo@bar.com"
           )

```

**Crossref search**

```{r}
#| label: crossref-search

requireNamespace("rcrossref")

#Crossref provides a mechanism to supply api-keys -- or to self-identify in order to enter the 'polite' pool of queries

Sys.setenv(crossref_email= query.ls[["EMAIL_str"]])

## crossref supports simple queries, terms are OR'ed together; can be filtered for time + publication
## See: https://api.crossref.org/swagger-ui/index.html#/Works/get_works

crossrefAuthorString <- function(x, maxauth=10) {
  if (is.null(x)) {
    return("")
  }
  if (! all(c(c("given","family")) %in% colnames(x))) {
        return("")

  }
  res <- x %>% 
    slice_head(n=maxauth) %>%
    select(given,family) %>%
    na.omit() %>%
    summarize(string = 
                glue::glue_collapse(sep=", ", 
                    glue::glue('{x} {y}',x=given, y=family))) %>%
    pull()
  
  if (nrow (x)>maxauth) {
    res <-  paste0(res,", et. al.")
  }
  res
}

crossref_query <- 
  glue::glue_collapse(
            glue::glue('"{x}"',
                       x= c(query.ls[["AND_terms"]])),
            sep="+")

# Note:
# limit too high results in server failure:
# Warning: 500 (server error): /works - org.apache.http.ContentTooLongException: entity content is too long [454985601] for the configured buffer limit [104857600]

crossref_response.ls <- 
  rcrossref::cr_works(
        sort="score",
        order="asc",
        filter=c("from_update_date"=as.character(query.ls[["FROM_date"]]),
                 "until_update_date"=as.character(query.ls[["UNTIL_date"]]),
                 "type"="book-chapter",
                 "type"="journal-article"
                ),
        query = crossref_query,          ,
        limit=100, # using a limit of 1000 fails with a server error 
        cursor="*",
        cursor_max=query.ls[["MAX_results"]]
  )

paste("Crossref total resultset size: ", 
      crossref_response.ls[["meta"]][["total_results"]])

crossref_results.df <- crossref_response.ls[["data"]]

crossref_results.df %<>% rowwise() %>% 
  mutate(author_str=crossrefAuthorString(author)) %>% ungroup()

rm()
```

\#

```{r more-searches}
#| label: openalex-search

options(openalexR.mailto = query.ls[["EMAIL_str"]])
requireNamespace("openalexR")

openAlexAuthorString <- function(x, maxauth=10) {
  if(!is.data.frame(x)) {
    return("")
  }
  res <- x %>% 
    slice_head(n=maxauth) %>%
    select(au_display_name) %>%
    na.omit() %>%
    summarize(string = 
                glue::glue_collapse(sep=", ", au_display_name)) %>%
    pull()
  
  if (nrow (x)>maxauth) {
    res <-  paste0(res,", et. al.")
  }
  res
}


openalex_query_str <-
  glue::glue('{x_and} {x_not} AND ({x_or})',
              x_or = glue::glue_collapse(
               glue::glue('"{x}"', x= query.ls[["OR_terms"]]),
               sep=" OR "),
              x_and = glue::glue_collapse(
               glue::glue('"{x}"', x = query.ls[["AND_terms"]]),
               sep=" AND "),
              x_not = glue::glue_collapse(
               glue::glue('NOT "{x}"', x = 
                            c(query.ls[["NOT_terms"]])),
                          ,
               sep=" "),
             
  )

openalex_query <- openalexR::oa_query(
  entity = "works",
  title_and_abstract.search = openalex_query_str,
  from_publication_date = as.character(query.ls[["FROM_date"]]),
  to_publication_date = as.character(query.ls[["UNTIL_date"]]),
  options = list(sort = "relevance_score:desc"),
  filter=list(type=c("article","book-chapter","review")),
  verbose=TRUE
)

openalex_response.count <- openalexR::oa_request(
  query_url = openalex_query,
  paging = "cursor",
  per_page = 200,
  pages=seq.int(from=1,to=max(1,query.ls[["MAX_results"]]/200)),
  verbose = TRUE,
  count_only=TRUE
)
paste(openalex_query_str)
paste("Open Alex Result Set N:", openalex_response.count[["count"]])


# see https://docs.openalex.org/how-to-use-the-api
openalex_response.ls <- openalexR::oa_request(
  query_url = openalex_query,
  paging = "cursor",
  per_page = 200,
  pages=seq.int(from=1,to=max(1,query.ls[["MAX_results"]]/200)),
  verbose = TRUE
)

openalex_results.df <- openalexR::oa2df(openalex_response.ls, entity = "works")

openalex_results.df %<>% 
  rowwise() %>% 
  mutate(author_str=openAlexAuthorString(author)) %>%
  ungroup()

rm(openalex_query_str)
```

**Post-process**

```{r}
#| label: merge-records

postFilter <- function(x, textfield,  filter.ls) {
  # TEST CASE
  # tibble(fruit) %>% mutate(i=row_number()) %>%
  #  postFilter(fruit, filter.ls =
  #  list(OR_terms=c("blue","black"),AND_terms=c("berry","err"),NOT_terms="straw"))
  
  res <- x
  
  if (!is.null(filter.ls[["NOT_terms"]])) {
    res %<>%
       dplyr::filter(!stringr::str_detect({{textfield}}, 
          glue::glue_collapse(filter.ls[["NOT_terms"]], sep="|")
      ))
  }
  
  res %<>% 
     dplyr::filter(stringr::str_detect({{textfield}}, 
        glue::glue_collapse(c(filter.ls[["OR_terms"]],filter.ls[["AND_terms"]]),
                            sep="|")
     ))
  
  for (i in filter.ls[["AND_terms"]]) {
    res %<>% 
      dplyr::filter(str_detect({{textfield}},i))
  }

  res
}

merged_min.df <-
  crossref_results.df %>% 
  select(doi, container.title, title, type, abstract, 
         pubdate = published.online, author_str ) %>%
  mutate(type = 
           case_match( 
             type,
             c("journal-article") ~ "article",
             .default = type
           ),
         pubdate = lubridate::as_date(pubdate)
  )

merged_min.df %<>% bind_rows(
  openalex_results.df %>%
    select(doi, container.title = so, title, type, abstract=ab, 
           pubdate=publication_date, author_str) %>%
    mutate(type = 
           case_match( 
             type,
             c("journal-article") ~ "article",
             .default = type
           ),
         pubdate = lubridate::as_date(pubdate)
  )
)

# drop duplicate DOI's

merged_min.df %<>%
  group_by(doi) %>%
  slice_head(n=1) %>%
  ungroup()
  

merged_min.df %<>%
  dplyr::filter(type %in% c("article","book-chapter","preprint","review")) %>%
  dplyr::filter(!is.na(doi))

# postfilter cross-ref

merged_min.df %<>% mutate(allMtext =
                           glue::glue("{x} {y}", x=title, y=abstract))

merged_min.df %<>% 
  postFilter(allMtext, filter.ls = query.ls)

save.image("checkpoint_bibsearch.RData")
```

**Export bibliography for zotero**

```{r}
#| label: export_records

requireNamespace("bib2df")

cur_date <- Sys.Date()

bibout.df <- 
  merged_min.df %>% 
  mutate(YEAR = lubridate::year(pubdate),
         CATEGORY = case_match(type, .default="MISC",
                c("article") ~ "ARTICLE",
                c("book-chapter") ~ "IN-COLLECTION",
                c("preprint") ~ "MISC"
                )
           ) %>%
  select(CATEGORY = type,
         TITLE = title, 
         DOI = doi, 
         YEAR,
         JOURNAL = container.title, 
         AUTHOR = author_str,
         WORKTYPE = type) %>%
  mutate(BIBTEXKEY=glue::glue('{x}{y}',
          x=stringr::str_trunc(stringr::str_remove_all(AUTHOR," |,"),
                               width=10, ellipsis=""),
          y=YEAR),
          DATEADDED = cur_date
         )


#bibtex version for zotero
bibout.df %>% bib2df::df2bib(file="bibout.bib")

merged_min.df %>% 
  pull(doi) %>%
  write_lines(file="dois.txt")
```

```{r}
#| label: googlesheets_update
#target google sheets

#rangestart <- "pubdata!A1"
targetsheet <- 'https://docs.google.com/spreadsheets/d/1F_ySl08aIMnkcayudIr7P5dyuB0rHKg9VZJKah6FloA/edit?gid=0#gid=0'
requireNamespace('googlesheets4')
sheetOut.df <- bibout.df %>%
  rename(pub_title=TITLE, pub_authors=AUTHOR, pub_year=YEAR, pub_type = WORKTYPE,
         pub_doi=DOI, metaDateAdded=DATEADDED ) %>%
  relocate(pub_title,	pub_authors,	pub_year,	pub_type,	pub_doi,	metaDateAdded)
  googlesheets4::sheet_append(ss=targetsheet, data=sheetOut.df) 
#googlesheets4::range_write(ss=targetsheet, range=rangestart, data=sheetOut.df) 
```

**Todo:**

-   doublecheck false positives (based on `postFilter` vs. `openAlex`

-   explore search term generation using `litsearchr`

-   forward citation snowballing

-   Lens / Dimensions searching using `dimensionsR` , `citationchaser`

-   export counts for (parts of) PRISMA chart

-   pull coding spreadsheet from gdrive for PRISMA charting and descriptives

-   push to Zotero
